modelVersion: 2.0

description: "Deploy SC"

import:
  base: rulesets/ruleset2.0-eric-base.yaml
## container rulesets
  cddjmxexp: eric-sc-cddjmxexporter/ruleset2.0.yaml
  certntf: eric-sc-certnotifier/ruleset2.0.yaml
  tapagent: eric-sc-tapagent/ruleset2.0.yaml
  tapcol: eric-sc-tapcollector/ruleset2.0.yaml
  tlskeylogagent: eric-sc-tlskeylogagent/ruleset2.0.yaml
  sds: eric-sc-sds/ruleset2.0.yaml
  logfwdr: eric-sc-logforwarder/ruleset2.0.yaml
  envoywrk: eric-sc-envoyworker/ruleset2.0.yaml
## common sub-chart rulesets
  scrlf: eric-sc-rlf/ruleset2.0.yaml
  scnlf: eric-sc-nlf/ruleset2.0.yaml
  scmonitor: eric-sc-monitor/ruleset2.0.yaml
## integration helm chart rulesets
  scbsf: eric-sc-bsf/ruleset2.0.yaml
  scscp: eric-sc-scp/ruleset2.0.yaml
  scsepp: eric-sc-sepp/ruleset2.0.yaml
  scspider: eric-sc-spider/ruleset2.0.yaml
# misc actions rulesets
  config: rulesets/ruleset2.0-eric-sc-configuration.yaml
properties:
  - CERTS_OUT_DIR: .certificates

env:
  - DDC (default=${env.config.LOGGING})
  - CONTOUR (default=true)
  - CONTOUR_MTLS (default=false)
  - SC_INGRESS_HOST (default="")
  - CM_GUI (default=false)
  - INTERNAL_CNCS_BASE_VALUES (default="csar/internal/eric-cloud-native-base-values.yaml")
  - INTERNAL_CNCS_NF_ADDITIONS_VALUES (default="csar/internal/eric-cloud-native-nf-additions-values.yaml")
  - INTERNAL_SC_CS_VALUES (default="csar/internal/eric-sc-cs-values.yaml")
  # Workaround for CI runs to force the use of node ports instead of loadbalancer
  # Prevents LB from assigning an external IP to the service
  - FORCE_SERVICE_TYPE_NODE_PORT (default=false)
  - DIVISION_METHOD (default=split)
  - DC1_NAMESPACE (default=false)
  - DC2_NAMESPACE (default=false)
  - GEORED (default=false)
  - BSF_DIAMETER (default=false)

var:
  - ingress-host
  - mvn-args
  - username

rules:

  init:
    - task: delete-output-dir
      cmd: rm -rf ${base.OUTPUT_DIR}/
    - rule: base.init
    - task: base.cncs-init:gerrit-password
    - task: set-ingressHost
      cmd:  /bin/bash -c 'RES="minikube";
                         [[ -z "${env.SC_INGRESS_HOST}" ]]
                         && [[ "${env.base.KUBE_HOST}" != "minikube" ]]
                         && BASE_HOST="${env.base.KUBE_HOST}.rnd.gic.ericsson.se"
                         && [[ ! -z "$BASE_HOST" ]]
                         && RES="${var.base.namespace}.$BASE_HOST";
                         [[ ! -z "${env.SC_INGRESS_HOST}" ]] && RES="$SC_INGRESS_HOST";
                         [[ "$RES" == "*" ]] && RES="";
                         echo "$RES" > ${base.OUTPUT_DIR}/var.ingress-host;'

  build:
    - task: esc-parent
      docker-image: base.java-maven-builder
      cmd: mvn --no-transfer-progress
               -settings ${var.base.build-proxy}
               -Duser.home=.
               ${var.base.mvn-args}
               ${base.MVN_ARGS}
               -Dmaven.deploy.skip=true
               -am -pl !:lm-lib,!:eric-chfsim,!:eric-nrfsim,!:eric-seppsim,!:eric-loadgen,!:vtaprecorder,!:eric-bsf-load package

  images-local:
    - rule: cddjmxexp.all-local
    - rule: certntf.all-local
    - rule: tapagent.all-local
    - rule: tapcol.all-local
    - rule: tlskeylogagent.all-local
    - rule: sds.all-local
    - rule: logfwdr.all-local
    - rule: envoywrk.all-local

  images-ci:
    - rule: cddjmxexp.all-ci
    - rule: certntf.all-ci
    - rule: tapagent.all-ci
    - rule: tapcol.all-ci
    - rule: tlskeylogagent.all-ci
    - rule: sds.all-ci
    - rule: logfwdr.all-ci
    - rule: envoywrk.all-ci

  charts-internal-local:
    - rule: scrlf.all-local
    - rule: scnlf.all-local
    - rule: scmonitor.all-local
    - rule: scspider.all-internal-local

  charts-internal-ci:
    - rule: scrlf.all-local
    - rule: scnlf.all-local
    - rule: scmonitor.all-local
    - rule: scspider.all-internal-ci

  charts-ci:
    - rule: scrlf.all-ci
    - rule: scnlf.all-local
    - rule: scmonitor.all-ci
    - rule: scspider.all-ci

  test:
    - task: test
      docker-image: base.adp-release-auto
      cmd: version-handler increment -h

  deploy-geored:
    - task: check-geored-ns
      cmd: /bin/bash -c 'scripts/bash/georedNamespace.sh ${env.DC1_NAMESPACE} ${env.DC2_NAMESPACE}'
    - task: switch-to-dc1
      cmd: /bin/bash -c 'echo ${env.DC1_NAMESPACE} > .bob/var.namespace; echo $(cut -d'-' -f3- <<<"${env.DC1_NAMESPACE}") > .bob/var.username; bob/bob init:set-ingressHost; sed -i 's/${env.DC2_NAMESPACE}/'${env.DC1_NAMESPACE}'/g' ${CERTS_OUT_DIR}/supreme.yaml;'
    - task: switch-to-dc2
      cmd: /bin/bash -c 'echo ${env.DC2_NAMESPACE} > .bob/var.namespace; echo $(cut -d'-' -f3- <<<"${env.DC2_NAMESPACE}") > .bob/var.username; bob/bob init:set-ingressHost; sed -i 's/${env.DC1_NAMESPACE}/'${env.DC2_NAMESPACE}'/g' ${CERTS_OUT_DIR}/supreme.yaml;'
    - task: config-geored
      docker-image: base.python3-builder
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=$PWD/${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: /bin/bash -c 'python3 scripts/setup_geored.py -dc1 ${env.DC1_NAMESPACE} -dc2 ${env.DC2_NAMESPACE} --diameter ${env.BSF_DIAMETER}'

  deploy:
    - task: cncs-b
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scspider.CNCS_BASE_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.cncs-base-chart-tgz}
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${env.INTERNAL_CNCS_BASE_VALUES}
                        -f ${base.OUTPUT_DIR}/${scspider.CNCS_BASE_NAME}-values-final.yaml
                        ${var.config.eric-cloud-native-base-profiles}
                        --set eric-tm-ingress-controller-cr.enabled=${env.CONTOUR}
                        --set eric-odca-diagnostic-data-collector.enabled=${env.DDC}
    - task: cncs-a
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scspider.CNCS_ADDITIONS_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.cncs-add-chart-tgz}
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${env.INTERNAL_CNCS_NF_ADDITIONS_VALUES}
                        -f ${base.OUTPUT_DIR}/${scspider.CNCS_ADDITIONS_NAME}-values-final.yaml
                        ${var.config.eric-cloud-native-nf-additions-profiles}
                        --set eric-cnom-server.features.serviceCommunicationProxy=${env.CM_GUI}
    - task: sc-cs
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scspider.SC_CS_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-cs-chart-tgz}
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${env.INTERNAL_SC_CS_VALUES}
                        -f ${base.OUTPUT_DIR}/${scspider.SC_CS_NAME}-values-final.yaml
                        ${var.config.eric-sc-cs-profiles}
                        --set ingress.nbi.tls.verifyClientCertificate="${env.CONTOUR_MTLS}"
    - task: bsf
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scbsf.CHART_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-bsf-chart-tgz}
                        --timeout 1500s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scbsf.CHART_NAME}-values-final.yaml
                        ${var.config.eric-sc-bsf-profiles}
                        $(if [ "${env.FORCE_SERVICE_TYPE_NODE_PORT}" == "true" ]; then
                        echo "--set eric-bsf.service.worker.annotations.cloudProviderLB.ccm\.ews\.io/enable=\\\"false\\\"";
                        fi;)
                        --set eric-bsf.tapcollector.tappedData.divisionMethod="${env.DIVISION_METHOD}"
    - task: scp
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scscp.CHART_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-scp-chart-tgz}
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scscp.CHART_NAME}-values-final.yaml
                        ${var.config.eric-sc-scp-profiles}
                        $(if [ "${env.FORCE_SERVICE_TYPE_NODE_PORT}" == "true" ]; then
                        echo "--set eric-scp.service.worker.annotations.cloudProviderLB.ccm\.ews\.io/enable=\\\"false\\\"";
                        fi;)
                        --set eric-scp.tapcollector.tappedData.divisionMethod="${env.DIVISION_METHOD}"
    - task: sepp
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scsepp.CHART_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-sepp-chart-tgz}
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scsepp.CHART_NAME}-values-final.yaml
                        ${var.config.eric-sc-sepp-profiles}
                        $(if [ "${env.FORCE_SERVICE_TYPE_NODE_PORT}" == "true" ]; then
                        echo "--set eric-sepp.service.worker.annotations.cloudProviderLB.ccm\.ews\.io/enable=\\\"false\\\"";
                        fi;)
                        --set eric-sepp.tapcollector.tappedData.divisionMethod="${env.DIVISION_METHOD}"
    - task: sc-dsc
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm install ${scspider.SC_DSC_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-dsc-chart-tgz}
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scspider.SC_DSC_NAME}-values-final.yaml
                        ${var.config.eric-dsc-profiles}

  post-deploy:
    - task: pm-httpproxy
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: sh scripts/httpproxy/pm_httpproxy.sh create ${env.base.KUBE_HOST}
    - task: search-engine-httpproxy
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: sh scripts/httpproxy/search_engine_httpproxy.sh create ${env.base.KUBE_HOST}
    - task: osmn-httpproxy
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: sh scripts/httpproxy/osmn_httpproxy.sh create ${env.base.KUBE_HOST} ${env.config.ROP_FILES_OPT2}
    - task: envoy-admin-httpproxy
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: sh scripts/httpproxy/worker-envoy-admin.sh create ${env.base.KUBE_HOST}

  upgrade:
    - task: cncs-b
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scspider.CNCS_BASE_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.cncs-base-chart-tgz}
                        --install
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${env.INTERNAL_CNCS_BASE_VALUES}
                        -f ${base.OUTPUT_DIR}/${scspider.CNCS_BASE_NAME}-values-final.yaml
                        ${var.config.eric-cloud-native-base-profiles}
                        --set eric-tm-ingress-controller-cr.enabled=${env.CONTOUR}
                        --set eric-odca-diagnostic-data-collector.enabled=${env.DDC}
    - task: cncs-a
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scspider.CNCS_ADDITIONS_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.cncs-add-chart-tgz}
                        --install
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${env.INTERNAL_CNCS_NF_ADDITIONS_VALUES}
                        -f ${base.OUTPUT_DIR}/${scspider.CNCS_ADDITIONS_NAME}-values-final.yaml
                        ${var.config.eric-cloud-native-nf-additions-profiles}
                        --set eric-cnom-server.features.serviceCommunicationProxy=${env.CM_GUI}
    - task: sc-cs
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scspider.SC_CS_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-cs-chart-tgz}
                        --install
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${env.INTERNAL_SC_CS_VALUES}
                        -f ${base.OUTPUT_DIR}/${scspider.SC_CS_NAME}-values-final.yaml
                        ${var.config.eric-sc-cs-profiles}
                        --set ingress.nbi.tls.verifyClientCertificate="${env.CONTOUR_MTLS}"
    - task: bsf
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scbsf.CHART_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-bsf-chart-tgz}
                        --install
                        --timeout 1500s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scbsf.CHART_NAME}-values-final.yaml
                        ${var.config.eric-sc-bsf-profiles}
                        --set eric-bsf.tapcollector.tappedData.divisionMethod="${env.DIVISION_METHOD}"
    - task: scp
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scscp.CHART_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-scp-chart-tgz}
                        --install
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scscp.CHART_NAME}-values-final.yaml
                        ${var.config.eric-sc-scp-profiles}
                        --set eric-scp.tapcollector.tappedData.divisionMethod="${env.DIVISION_METHOD}"
    - task: sepp
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scsepp.CHART_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-sepp-chart-tgz}
                        --install
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scsepp.CHART_NAME}-values-final.yaml
                        ${var.config.eric-sc-sepp-profiles}
                        --set eric-sepp.tapcollector.tappedData.divisionMethod="${env.DIVISION_METHOD}"
    - task: sc-dsc
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm upgrade ${scspider.SC_DSC_NAME}-${var.username} ${base.OUTPUT_DIR}/${scspider.CHART_DEPENDENCIES_FOLDER}/${var.scspider.sc-dsc-chart-tgz}
                        --install
                        --timeout 900s
                        --namespace ${var.base.namespace}
                        -f ${base.OUTPUT_DIR}/${scspider.SC_DSC_NAME}-values-final.yaml
                        ${var.config.eric-dsc-profiles}

  undeploy:
    - task: cncs-b
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scspider.CNCS_BASE_NAME}-${var.username} --namespace ${var.base.namespace}
    - task: cncs-a
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scspider.CNCS_ADDITIONS_NAME}-${var.username} --namespace ${var.base.namespace}
    - task: sc-cs
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scspider.SC_CS_NAME}-${var.username} --namespace ${var.base.namespace}
    - task: bsf
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scbsf.CHART_NAME}-${var.username} --namespace ${var.base.namespace}
    - task: scp
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scscp.CHART_NAME}-${var.username} --namespace ${var.base.namespace}
    - task: sepp
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scsepp.CHART_NAME}-${var.username} --namespace ${var.base.namespace}
    - task: sc-dsc
      docker-image: base.helm-builder-py3
      docker-envs:
        - HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}
        - KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf
      cmd: helm delete ${scspider.SC_DSC_NAME}-${var.username} --namespace ${var.base.namespace}

  clean:
    - task: delete-namespace
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'kubectl get namespace ${var.base.namespace};
                  if [ $? -eq 0 ]; then
                     echo "Deleting namespace ${var.base.namespace}";
                     kubectl delete namespace ${var.base.namespace};
                     if [ $? -eq 0 ]; then
                        echo "Successfully deleted namespace ${var.base.namespace}";
                     else
                        echo "Failed to delete namespace ${var.base.namespace}";
                        exit 1;
                     fi
                  else
                     echo "No need to clean namespace ${var.base.namespace} because it does not exist";
                  fi'
    - task: create-namespace
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'kubectl get namespace ${var.base.namespace};
                  if [ $? -eq 0 ]; then
                     echo "Namespace ${var.base.namespace} already exists";
                  else
                     echo "Creating namespace ${var.base.namespace}";
                     kubectl create namespace ${var.base.namespace};
                     if [ $? -ne 0 ]; then
                        echo "Failed to create namespace ${var.base.namespace}";
                     fi
                  fi'
    - task: cncs-cluster-resources
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'echo "Clean cluster resources for both CNCS Base and CNCS NF Additions";
                  resources=$(kubectl api-resources --verbs=list --namespaced=false -o name);
                  for resource in $resources; do
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.CNCS_BASE_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scspider.CNCS_BASE_NAME}-${var.username}:\n$instances";
                          echo "Clean $resource resources with label ${scspider.CNCS_BASE_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.CNCS_BASE_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.CNCS_ADDITIONS_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scspider.CNCS_ADDITIONS_NAME}-${var.username}:\n$instances";
                          echo "Clean $resource resources with label ${scspider.CNCS_ADDITIONS_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.CNCS_ADDITIONS_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                  done'
    - task: bsf-cluster-resources
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'echo "Clean cluster resources with label ${scbsf.CHART_NAME}-${var.username}";
                  resources=$(kubectl api-resources --verbs=list --namespaced=false -o name);
                  for resource in $resources; do
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scbsf.CHART_NAME}-${var.username}  --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scbsf.CHART_NAME}-${var.username}\n$instances";
                          echo "Clean $resource resources with label ${scbsf.CHART_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scbsf.CHART_NAME}-${var.username}  --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                  done'
    - task: scp-cluster-resources
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'echo "Clean cluster resources with label ${scscp.CHART_NAME}-${var.username}";
                  resources=$(kubectl api-resources --verbs=list --namespaced=false -o name);
                  for resource in $resources; do
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scscp.CHART_NAME}-${var.username}  --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scscp.CHART_NAME}-${var.username}\n$instances";
                          echo "Clean $resource resources with label ${scscp.CHART_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scscp.CHART_NAME}-${var.username}  --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                  done'
    - task: sepp-cluster-resources
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'echo "Clean cluster resources with label ${scsepp.CHART_NAME}-${var.username}";
                  resources=$(kubectl api-resources --verbs=list --namespaced=false -o name);
                  for resource in $resources; do
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scsepp.CHART_NAME}-${var.username}  --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scsepp.CHART_NAME}-${var.username}\n$instances";
                          echo "Clean $resource resources with label ${scsepp.CHART_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scsepp.CHART_NAME}-${var.username}  --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                  done'
    - task: dsc-cluster-resources
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'echo "Clean cluster resources with label ${scspider.SC_DSC_NAME}-${var.username}";
                  resources=$(kubectl api-resources --verbs=list --namespaced=false -o name);
                  for resource in $resources; do
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.SC_DSC_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scspider.SC_DSC_NAME}-${var.username}\n$instances";
                          echo "Clean $resource resources with label ${scspider.SC_DSC_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.SC_DSC_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                  done'
    - task: sc-cs-cluster-resources
      docker-image: base.helm-builder-py3
      docker-flags:
        - "--env HELM_VERSION=${base.HELM_VERSION_HELMBUILDERPY3}"
        - "--env KUBECONFIG=${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf"
      cmd: sh -c 'echo "Clean cluster resources with label ${scspider.SC_CS_NAME}-${var.username}";
                  resources=$(kubectl api-resources --verbs=list --namespaced=false -o name);
                  for resource in $resources; do
                      instances=$(kubectl get $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.SC_CS_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A);
                      if [ -n "$instances" ]; then
                          echo -e "Found instances with label ${scspider.SC_CS_NAME}-${var.username}\n$instances";
                          echo "Clean $resource resources with label ${scspider.SC_CS_NAME}-${var.username}";
                          kubectl delete $resource --ignore-not-found=true --selector=app.kubernetes.io/instance=${scspider.SC_CS_NAME}-${var.username} --kubeconfig ${base.OUTPUT_DIR}/${env.base.KUBE_HOST}.admin.conf -A;
                          if [ $? -eq 0 ]; then
                             echo "Successfully clean $resource resources";
                          else
                             echo "Clean of $resource resources failed!";
                          fi;
                      fi;
                  done'